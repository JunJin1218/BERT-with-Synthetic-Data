# Pre-setting
model: bert-large-cased
benchmark: biosses
task: biosses
seed: 42

# Data ratio
data_model: gpt-5.1
mode: mix # original, synthetic, mix
scale: 1        # n * original data

# Data preparation
dataset: "synthetic-5-100.jsonl"
truncation: true
padding: max_length
max_length: 256

# Hyperparameters
eval_strategy: epoch
save_strategy: epoch
learning_rate: 2e-5
save_total_limit: 5
per_device_train_batch_size: 32
per_device_eval_batch_size: 32
num_train_epochs: 10
weight_decay: 0.01
load_best_model_at_end: true
metric_for_best_model: pearson
greater_is_better: true
gradient_accumulation_steps: 1
